{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from os.path import join as pjoin\n",
    "import h5py\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from tqdm import tqdm\n",
    "import mne\n",
    "from mne.io import RawArray\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "from os import makedirs\n",
    "from os.path import join as pjoin\n",
    "from shutil import copy2, move\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from braindecode.datautil.signal_target import SignalAndTarget\n",
    "from braindecode.models.deep4 import Deep4Net\n",
    "from braindecode.torch_ext.optimizers import AdamW\n",
    "from braindecode.torch_ext.util import set_random_seeds\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ],
   "id": "696782bc0cb054"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class EEGDataProcessor:\n",
    "    def __init__(self, sampling_rate=250):\n",
    "        self.sfreq = sampling_rate\n",
    "        self.ch_types = ['eeg'] * 32\n",
    "        self.filter_bands = [\n",
    "            (4, 8), (8, 12), (12, 16), (16, 20), \n",
    "            (20, 24), (24, 28), (28, 32), (32, 36), (36, 40)\n",
    "        ]\n",
    "\n",
    "    def create_raw_from_data(self, data, ch_names):\n",
    "        info = mne.create_info(\n",
    "            ch_names=ch_names,\n",
    "            sfreq=self.sfreq,\n",
    "            ch_types=self.ch_types[:len(ch_names)]\n",
    "        )\n",
    "        return mne.io.RawArray(data, info)\n",
    "\n",
    "    def apply_filter_bank(self, raw):\n",
    "        filtered_data = []\n",
    "        for low_freq, high_freq in self.filter_bands:\n",
    "            raw_filtered = raw.copy()\n",
    "            raw_filtered.filter(low_freq, high_freq, method='iir')\n",
    "            filtered_data.append(raw_filtered.get_data())\n",
    "        return np.stack(filtered_data, axis=-1)"
   ],
   "id": "89764344adab58b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_shu_data(subj, processor):\n",
    "    file_path = os.getcwd()\n",
    "    data_list = []\n",
    "    labels = np.empty(0)\n",
    "    \n",
    "    # 上海大学通道名\n",
    "    ch_names = [\"Fp1\", \"Fp2\", \"Fz\", \"F3\", \"F4\", \"F7\", \"F8\", \"FC1\", \"FC2\", \"FC5\",\n",
    "                \"FC6\", \"Cz\", \"C3\", \"C4\", \"T3\", \"T4\", \"A1\", \"A2\", \"CP1\", \"CP2\",\n",
    "                \"CP5\", \"CP6\", \"Pz\", \"P3\", \"P4\", \"T5\", \"T6\", \"PO3\", \"PO4\", \"Oz\",\n",
    "                \"O1\", \"O2\"]\n",
    "    \n",
    "    # 删除的通道\n",
    "    channels_to_delete = [16, 17, 27, 28]\n",
    "    keep_channels = np.ones(len(ch_names), dtype=bool)\n",
    "    keep_channels[channels_to_delete] = False\n",
    "    ch_names = [ch for i, ch in enumerate(ch_names) if keep_channels[i]]\n",
    "    \n",
    "    for session in range(1, 6):\n",
    "        da = sio.loadmat(pjoin(file_path, 'SHU_Dataset', \n",
    "                              f'sub-{str(subj).zfill(3)}_ses-{str(session).zfill(2)}_task_motorimagery_eeg.mat'))\n",
    "        print(da['data'].shape)\n",
    "        # 对每个样本\n",
    "        for trial_idx in range(da['data'].shape[0]):\n",
    "            trial_data = da['data'][trial_idx]  # (channels, time)\n",
    "            trial_data = trial_data[keep_channels]\n",
    "            print(trial_data.shape)\n",
    "            # 滤波\n",
    "            raw = processor.create_raw_from_data(trial_data, ch_names)\n",
    "            filtered_data = processor.apply_filter_bank(raw)\n",
    "            data_list.append(filtered_data)\n",
    "            \n",
    "        labels = np.hstack((labels, np.ravel(da['labels'])))\n",
    "    \n",
    "    X = np.stack(data_list, axis=0)\n",
    "    Y = (np.ravel(labels) - 1).astype(np.int64)\n",
    "    return X, Y"
   ],
   "id": "f93cbccc5843b4b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_stroke_data(subj, processor):\n",
    "    file_path = os.getcwd()\n",
    "    \n",
    "    # 中风通道名\n",
    "    ch_names = [\"Fp1\", \"Fp2\", \"Fz\", \"F3\", \"F4\", \"F7\", \"F8\", \"FCz\", \"FC3\", \"FC4\", \"FT7\",\n",
    "                \"FT8\", \"Cz\", \"C3\", \"C4\", \"T3\", \"T4\", \"CPz\", \"CP3\", \"CP4\", \"TP7\", \"TP8\",\n",
    "                \"Pz\", \"P3\", \"P4\", \"T5\", \"T6\", \"Oz\", \"O1\", \"O2\", \"HEOL\", \"VEOR\", \"Tng\"]\n",
    "    \n",
    "    # 删除的通道\n",
    "    channels_to_delete = [7, 17, 30, 31, 32]\n",
    "    keep_channels = np.ones(len(ch_names), dtype=bool)\n",
    "    keep_channels[channels_to_delete] = False\n",
    "    ch_names = [ch for i, ch in enumerate(ch_names) if keep_channels[i]]\n",
    "    \n",
    "    da = sio.loadmat(pjoin(file_path, 'Stroke_Dataset', \n",
    "                          f'sub-{str(subj).zfill(2)}',\n",
    "                          f'sub-{str(subj).zfill(2)}_task-motor-imagery_eeg.mat'))\n",
    "    \n",
    "    data = da['eeg']['rawdata'][0, 0]\n",
    "    labels = np.ravel(da['eeg']['label'][0, 0])\n",
    "    \n",
    "    # 采样率为500Hz，只要一半250Hz\n",
    "    data = data[:, :, 1000:3000:2]  # (trials, channels, time)\n",
    "    data_list = []\n",
    "    \n",
    "    print(data.shape)\n",
    "    \n",
    "    for trial_idx in range(data.shape[0]):\n",
    "        trial_data = data[trial_idx]  # (channels, time)\n",
    "        trial_data = trial_data[keep_channels]\n",
    "        print(trial_data.shape)\n",
    "        # 滤波\n",
    "        raw = processor.create_raw_from_data(trial_data, ch_names)\n",
    "        filtered_data = processor.apply_filter_bank(raw)\n",
    "        data_list.append(filtered_data)\n",
    "    \n",
    "    X = np.stack(data_list, axis=0)\n",
    "    Y = (labels - 1).astype(np.int64)\n",
    "    return X, Y\n"
   ],
   "id": "9eedd966d5295f34"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 初始化滤波器\n",
    "processor = EEGDataProcessor(sampling_rate=250)"
   ],
   "id": "3471b776e107820f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 处理上海大学数据\n",
    "print(\"Processing SHU Dataset...\")\n",
    "with h5py.File('data/SHU_data.h5', 'w') as f:\n",
    "    for subj in tqdm(range(1, 26)):\n",
    "        X, Y = get_shu_data(subj, processor)\n",
    "        \n",
    "        f.create_dataset('s' + str(subj) + '/X', data=X)\n",
    "        f.create_dataset('s' + str(subj) + '/Y', data=Y)\n"
   ],
   "id": "43416090eded1e8d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 处理中风数据\n",
    "print(\"\\nProcessing Stroke Dataset...\")\n",
    "with h5py.File('data/Stroke_data.h5', 'w') as f:\n",
    "    for subj in tqdm(range(1, 51)):\n",
    "        X, Y = get_stroke_data(subj, processor)\n",
    "\n",
    "        f.create_dataset('s' + str(subj) + '/X', data=X)\n",
    "        f.create_dataset('s' + str(subj) + '/Y', data=Y)"
   ],
   "id": "2d7ee0eaedd4927"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 标记完成\n",
    "print('done')"
   ],
   "id": "bb33a06cde9cad54"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 定义FBCNet（从上海大学获取）\n",
    "\"\"\"\n",
    "All network architectures: FBCNet, EEGNet, DeepConvNet\n",
    "@author: Ravikiran Mane\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "current_module = sys.modules[__name__]\n",
    "\n",
    "debug = False\n",
    "\n",
    "class Conv2dWithConstraint(nn.Conv2d):\n",
    "    def __init__(self, *args, doWeightNorm = True, max_norm=1, **kwargs):\n",
    "        self.max_norm = max_norm\n",
    "        self.doWeightNorm = doWeightNorm\n",
    "        super(Conv2dWithConstraint, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.doWeightNorm:\n",
    "            self.weight.data = torch.renorm(\n",
    "                self.weight.data, p=2, dim=0, maxnorm=self.max_norm\n",
    "            )\n",
    "        return super(Conv2dWithConstraint, self).forward(x)\n",
    "\n",
    "class LinearWithConstraint(nn.Linear):\n",
    "    def __init__(self, *args, doWeightNorm = True, max_norm=1, **kwargs):\n",
    "        self.max_norm = max_norm\n",
    "        self.doWeightNorm = doWeightNorm\n",
    "        super(LinearWithConstraint, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.doWeightNorm:\n",
    "            self.weight.data = torch.renorm(\n",
    "                self.weight.data, p=2, dim=0, maxnorm=self.max_norm\n",
    "            )\n",
    "        return super(LinearWithConstraint, self).forward(x)\n",
    "\n",
    "class VarLayer(nn.Module):\n",
    "    '''\n",
    "    The variance layer: calculates the variance of the data along given 'dim'\n",
    "    '''\n",
    "    def __init__(self, dim):\n",
    "        super(VarLayer, self).__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.var(dim = self.dim, keepdim= True)\n",
    "\n",
    "class FBCNet(nn.Module):\n",
    "    '''\n",
    "        Just a FBCSP like structure : Channel-wise convolution and then variance along the time axis\n",
    "        The data input is in a form of batch x 1 x chan x time x filterBand\n",
    "    '''\n",
    "\n",
    "    def SCB(self, m, nChan, nBands, doWeightNorm=True, *args, **kwargs):\n",
    "        '''\n",
    "        The spatial convolution block\n",
    "        m : number of spatial filters.\n",
    "        nBands: number of bands in the data\n",
    "        '''\n",
    "        return nn.Sequential(\n",
    "            Conv2dWithConstraint(nBands, m * nBands, (nChan, 1), groups=nBands,\n",
    "                                 max_norm=2, doWeightNorm=doWeightNorm, padding=0),\n",
    "            nn.BatchNorm2d(m * nBands),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "    def LastBlock(self, inF, outF, doWeightNorm=True, *args, **kwargs):\n",
    "        return nn.Sequential(\n",
    "            LinearWithConstraint(inF, outF, max_norm=0.5, doWeightNorm=doWeightNorm, *args, **kwargs),\n",
    "            nn.LogSoftmax(dim=1))\n",
    "\n",
    "    def __init__(self, nChan, nTime, nClass=2, nBands=9, m=4,\n",
    "                 temporalLayer='VarLayer', doWeightNorm=True, *args, **kwargs):\n",
    "        super(FBCNet, self).__init__()\n",
    "\n",
    "        self.nBands = nBands\n",
    "        self.m = m\n",
    "\n",
    "        # create all the parallel SCBc\n",
    "        self.scb = self.SCB(m, nChan, self.nBands, doWeightNorm=doWeightNorm)\n",
    "\n",
    "        # Formulate the temporal aggregator\n",
    "        self.temporalLayer = current_module.__dict__[temporalLayer](dim=3)\n",
    "\n",
    "        # The final fully connected layer\n",
    "        self.lastLayer = self.LastBlock(self.m * self.nBands, nClass, doWeightNorm=doWeightNorm)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 根据我的数据处理一下形状\n",
    "        # print(x.shape)\n",
    "        x = x.unsqueeze(1)\n",
    "        # print(x.shape)\n",
    "        x = torch.squeeze(x.permute((0, 4, 2, 3, 1)), dim=4)\n",
    "        \n",
    "        x = self.scb(x)\n",
    "        x = self.temporalLayer(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.lastLayer(x)\n",
    "        return x\n"
   ],
   "id": "4d45cfdb97d77ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 设置日志格式\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                    level=logging.INFO, stream=sys.stdout)\n",
    "\n",
    "# 配置参数\n",
    "datapath = os.path.join(os.getcwd(), 'data','SHU_data.h5')  # 数据文件路径\n",
    "strokepath = os.path.join(os.getcwd(), 'data','Stroke_data.h5') # 中风患者数据文件路径\n",
    "outpath = os.path.join(os.getcwd(), 'results')         # 结果输出路径\n",
    "if torch.cuda.is_available():\n",
    "    gpu_device = 0\n",
    "    device = 'cuda'\n",
    "    torch.cuda.set_device(gpu_device)\n",
    "else:\n",
    "    gpu_device = 'cpu'\n",
    "    device = 'cpu'\n",
    "\n",
    "# 设置设备\n",
    "torch.cuda.set_device(gpu_device)\n",
    "\n",
    "# 设置随机种子\n",
    "set_random_seeds(seed=20200205, cuda=True)\n",
    "\n",
    "# 训练参数\n",
    "BATCH_SIZE = 32\n",
    "TRAIN_EPOCH = 350\n",
    "\n",
    "# 获取单个被试数据\n",
    "def get_data(dfile, subj):\n",
    "    dpath = '/s' + str(subj)\n",
    "    X = dfile[dpath]['X']\n",
    "    Y = dfile[dpath]['Y']\n",
    "    return X, Y\n",
    "\n",
    "# 获取多个被试数据\n",
    "def get_multi_data(dfile, subjs):\n",
    "    Xs = []\n",
    "    Ys = []\n",
    "    for s in subjs:\n",
    "        x, y = get_data(dfile, s)\n",
    "        Xs.append(x[:])\n",
    "        Ys.append(y[:])\n",
    "    X = np.concatenate(Xs, axis=0)\n",
    "    Y = np.concatenate(Ys, axis=0)\n",
    "    return X, Y\n",
    "\n",
    "# 训练base model\n",
    "def train_model(fold,lr,X_health, Y_health):\n",
    "    outpath = pjoin('results', 'S'+str(fold+1).zfill(2))\n",
    "    makedirs(outpath, exist_ok=True)\n",
    "    \n",
    "    # 中风被试编号\n",
    "    all_subjs = list(range(1, 51))\n",
    "    subjs = list(range(1, 51))\n",
    "    \n",
    "    # 确定被试和交叉验证集\n",
    "    test_subj = subjs[fold]\n",
    "    cv_set = np.array(all_subjs[fold+1:] + all_subjs[:fold])\n",
    "    ###################################################\n",
    "    # cv_set = np.array(subjs[-1:] + subjs[:1])\n",
    "    \n",
    "\n",
    "    # 创建6折交叉验证\n",
    "    kf = KFold(n_splits=6)\n",
    "    ###################################################\n",
    "    # kf = KFold(n_splits=2)\n",
    "    \n",
    "    # 打开中风数据文件\n",
    "    print('start base model training')\n",
    "    with h5py.File(strokepath, 'r') as sfile:\n",
    "        cv_loss = []\n",
    "        \n",
    "        # 交叉验证\n",
    "        for cv_index, (train_index, test_index) in enumerate(kf.split(cv_set)):\n",
    "            # 准备训练、验证和测试集\n",
    "            train_subjs = cv_set[train_index]\n",
    "            valid_subjs = cv_set[test_index]\n",
    "            \n",
    "            X_train, Y_train = get_multi_data(sfile, train_subjs)\n",
    "            X_val, Y_val = get_multi_data(sfile, valid_subjs)\n",
    "            X_test, Y_test = get_data(sfile, test_subj)\n",
    "            \n",
    "            # 扩充训练集\n",
    "            X_train = np.concatenate((X_train, X_health), axis=0)\n",
    "            Y_train = np.concatenate((Y_train, Y_health), axis=0)\n",
    "    \n",
    "            # 扩充验证集\n",
    "            X_val = np.concatenate((X_val, X_health), axis=0)\n",
    "            Y_val = np.concatenate((Y_val, Y_health), axis=0)\n",
    "            \n",
    "            # 转换为张量\n",
    "            X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "            Y_train = torch.tensor(Y_train, dtype=torch.long)\n",
    "            X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "            Y_val = torch.tensor(Y_val, dtype=torch.long)\n",
    "            X_test = torch.tensor(X_test[:], dtype=torch.float32)\n",
    "            Y_test = torch.tensor(Y_test[:], dtype=torch.long)\n",
    "            \n",
    "            # 将数据移到GPU\n",
    "            X_train = X_train.to(device)\n",
    "            Y_train = Y_train.to(device)\n",
    "            X_val = X_val.to(device)\n",
    "            Y_val = Y_val.to(device)\n",
    "            X_test = X_test.to(device)\n",
    "            Y_test = Y_test.to(device)\n",
    "            \n",
    "            # 创建DataLoader\n",
    "            train_dataset = TensorDataset(X_train, Y_train)\n",
    "            valid_dataset = TensorDataset(X_val, Y_val)\n",
    "            test_dataset = TensorDataset(X_test, Y_test)\n",
    "            \n",
    "            train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "            valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "            # print('len(test_loader)',len(test_loader))\n",
    "            \n",
    "            # 模型参数\n",
    "            n_classes = 2\n",
    "            in_chans = X_train.shape[1]\n",
    "            # print(in_chans)\n",
    "            \n",
    "            # 创建模型\n",
    "            model = FBCNet(\n",
    "                nChan=in_chans,\n",
    "                nClass=n_classes,\n",
    "                nTime=X_train.shape[2],\n",
    "                nBands=9\n",
    "            ).to(device)\n",
    "            \n",
    "            # 训练参数\n",
    "            optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0.5*0.0001)\n",
    "            scheduler = CosineAnnealingLR(optimizer, T_max=TRAIN_EPOCH)\n",
    "            criterion = F.nll_loss\n",
    "            \n",
    "            # 训练\n",
    "            best_val_loss = float('inf')\n",
    "            best_val_acc = float('inf')\n",
    "            for epoch in range(TRAIN_EPOCH):\n",
    "                model.train()\n",
    "                running_loss = 0.0\n",
    "                for inputs, labels in train_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # print(inputs.shape, labels.shape)\n",
    "                    \n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss += loss.item()\n",
    "                    \n",
    "                scheduler.step()\n",
    "                \n",
    "                # 验证过程\n",
    "                model.eval()\n",
    "                \n",
    "                train_acc = 0.0\n",
    "                with torch.no_grad():\n",
    "                    for inputs, labels in train_loader:\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        outputs = model(inputs)\n",
    "                        _, predicted = torch.max(outputs, 1)\n",
    "                        train_acc += (predicted == labels).float().sum().item()\n",
    "\n",
    "                        \n",
    "                \n",
    "                val_loss = 0.0\n",
    "                val_acc = 0.0\n",
    "                with torch.no_grad():\n",
    "                    for inputs, labels in valid_loader:\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        val_loss += loss.item()\n",
    "                        _, predicted = torch.max(outputs, 1)\n",
    "                        val_acc += (predicted == labels).float().sum().item()\n",
    "\n",
    "                # 保存最佳模型\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_val_acc = val_acc\n",
    "                    torch.save(model.state_dict(), pjoin(outpath, f'best_model_f{fold}_cv{cv_index}.pt'))\n",
    "\n",
    "                # 打印训练和验证结果\n",
    "                if epoch % 20 == 0:\n",
    "                    print(f'Epoch {epoch+1}/{TRAIN_EPOCH}, Training Loss: {running_loss/len(train_loader)}, Training Acc: {train_acc/len(train_loader.dataset)}, Validation Loss: {val_loss/len(valid_loader)}, Validation Acc: {val_acc/len(valid_loader.dataset)}')\n",
    "            \n",
    "            # 评估测试集\n",
    "            model = FBCNet(\n",
    "                nChan=in_chans,\n",
    "                nClass=n_classes,\n",
    "                nTime=X_train.shape[2],\n",
    "                nBands=9\n",
    "            ).to(device)\n",
    "            model.load_state_dict(torch.load(pjoin(outpath, f'best_model_f{fold}_cv{cv_index}.pt')))\n",
    "            model.eval()\n",
    "            test_loss = 0.0\n",
    "            test_acc = 0.0\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in test_loader:\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    test_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    test_acc += (predicted == labels).float().sum().item()\n",
    "                    \n",
    "            \n",
    "            with open(pjoin(outpath, f'test_base_s{test_subj}_f{fold}_cv{cv_index}.json'), 'w') as f:\n",
    "                json.dump({'test_loss': test_loss/len(test_loader), 'test_acc': test_acc/len(test_loader.dataset)}, f)\n",
    "            \n",
    "            cv_loss.append(best_val_loss)\n",
    "\n",
    "        # 选择最佳交叉验证模型\n",
    "        best_cv = np.argmin(cv_loss)\n",
    "        best_dir = pjoin(outpath, \"best\")\n",
    "        os.makedirs(best_dir, exist_ok=True)\n",
    "        \n",
    "        # 记录折数\n",
    "        with open(pjoin(best_dir, \"fold_bestcv.txt\"), 'w') as f:\n",
    "            f.write(f\"{fold}, {best_cv}\\n\")\n",
    "        \n",
    "        # 复制文件到best目录\n",
    "        copy2(pjoin(outpath, f'best_model_f{fold}_cv{best_cv}.pt'),\n",
    "              pjoin(best_dir, f'model_f{fold}.pt'))\n",
    "        \n",
    "        copy2(pjoin(outpath, f'test_base_s{test_subj}_f{fold}_cv{best_cv}.json'),\n",
    "              pjoin(best_dir, f'test_base_s{test_subj}_f{fold}.json'))\n",
    "\n"
   ],
   "id": "c4a3260d08de14a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "health_set = np.array(list(range(1,6))) # 使用上海大学的前5个被试（否则服务器跑不动）\n",
    "with h5py.File(datapath, 'r') as dfile:\n",
    "        X_health, Y_health = get_multi_data(dfile, health_set)\n",
    "# 设置折数（测试的被试数量）\n",
    "folds = 50\n",
    "###################################################\n",
    "# folds = 1\n",
    "\n",
    "lr = 1*0.001\n",
    "for fold in range(folds):\n",
    "    print(f\"开始训练第 {fold+1} 折\")\n",
    "    train_model(fold,lr,X_health, Y_health)\n",
    "    print(f\"第 {fold+1} 折训练完成\")"
   ],
   "id": "f221c1670fa72079"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 标记完成\n",
    "print(\"base model training is done\")"
   ],
   "id": "66f675c40fece24c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# 获取当前工作目录\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# 创建目标文件夹路径\n",
    "models_dir = os.path.join(cwd, 'models')\n",
    "results_dir = os.path.join(cwd, 'results')\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "# 循环处理S01到S50\n",
    "for i in range(1, 51):\n",
    "    folder_name = f'S{i:02}'\n",
    "    source_folder = os.path.join(results_dir, folder_name, 'best')\n",
    "\n",
    "    # 检查文件夹是否存在\n",
    "    if os.path.exists(source_folder):\n",
    "        # 列出所有model开头，.pt结尾的文件\n",
    "        for file_name in os.listdir(source_folder):\n",
    "            if file_name.startswith('model') and file_name.endswith('.pt'):\n",
    "                source_file = os.path.join(source_folder, file_name)\n",
    "                destination_file = os.path.join(models_dir, file_name)\n",
    "                # 复制文件\n",
    "                shutil.copy(source_file, destination_file)\n",
    "                print(f'Copied {source_file} to {destination_file}')\n",
    "    else:\n",
    "        print(f'Folder {source_folder} does not exist')"
   ],
   "id": "2788c606524a9502"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "datapath = os.path.join(os.getcwd(),'data','SHU_data.h5')\n",
    "strokepath = os.path.join(os.getcwd(), 'data','Stroke_data.h5') # 中风患者数据文件路径\n",
    "outpath = os.path.join(os.getcwd(), 'results')         # 结果输出路径\n",
    "modelpath = os.path.join(os.getcwd(),'models')    # 模型路径\n",
    "ADAPT_EPOCH = 50\n",
    "subjs = list(range(1, 51))\n",
    "################################################\n",
    "# subjs = list(range(1, 2))\n",
    "def adaptive_train(modelpath, subjs, scheme=4, lr=0.0005):\n",
    "    results = []\n",
    "    \n",
    "    for fold, subj in enumerate(subjs):\n",
    "        # 加载预训练检查点\n",
    "        checkpoint = torch.load(\n",
    "            os.path.join(modelpath, f'model_f{fold}.pt'), \n",
    "            map_location=f'cuda:{torch.cuda.current_device()}'\n",
    "        )\n",
    "    \n",
    "        with h5py.File(strokepath, 'r') as sfile:\n",
    "            # 获取数据\n",
    "            X, Y = get_data(sfile, subj)\n",
    "            \n",
    "            # 初始化模型\n",
    "            model = FBCNet(\n",
    "                nChan=X.shape[1], \n",
    "                nTime=X.shape[2], \n",
    "                nClass=2, \n",
    "                nBands=9\n",
    "            ).cuda()\n",
    "            \n",
    "            # 加载预训练权重\n",
    "            model.load_state_dict(checkpoint)\n",
    "            \n",
    "            # 按自适应策略冻结/解冻层\n",
    "            if scheme != 5:\n",
    "                # 冻结所有层\n",
    "                for param in model.parameters():\n",
    "                    param.requires_grad = False\n",
    "    \n",
    "                if scheme in {1, 2, 3, 4}:\n",
    "                    # 解冻分类器层\n",
    "                    for param in model.lastLayer.parameters():\n",
    "                        param.requires_grad = True\n",
    "    \n",
    "                if scheme in {2, 3, 4}:\n",
    "                    # 解冻conv4层\n",
    "                    for param in model.scb.parameters():\n",
    "                        param.requires_grad = True\n",
    "    \n",
    "            # 优化器\n",
    "            optimizer = torch.optim.AdamW(\n",
    "                filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                lr=lr, weight_decay=0.5*0.001\n",
    "            )\n",
    "            criterion = nn.NLLLoss()\n",
    "            \n",
    "            # 训练和评估\n",
    "            cv = StratifiedKFold(n_splits=10, shuffle=False)\n",
    "            cv_acc = []\n",
    "            \n",
    "            for cv_fold, (train_idx, test_idx) in enumerate(cv.split(X, Y)):\n",
    "                X_train, X_test = X[train_idx], X[test_idx]\n",
    "                Y_train, Y_test = Y[train_idx], Y[test_idx]\n",
    "                X_train_tensor = torch.FloatTensor(X_train).cuda()\n",
    "                Y_train_tensor = torch.LongTensor(Y_train.astype(np.int64)).cuda()\n",
    "                X_test_tensor = torch.FloatTensor(X_test).cuda()\n",
    "                Y_test_tensor = torch.LongTensor(Y_test.astype(np.int64)).cuda()\n",
    "                \n",
    "                # 训练模型\n",
    "                model.train()\n",
    "                for epoch in range(ADAPT_EPOCH):\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(X_train_tensor)\n",
    "                    loss = criterion(outputs, Y_train_tensor)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # 评估模型\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    test_outputs = model(X_test_tensor)\n",
    "                    _, predicted = torch.max(test_outputs, 1)\n",
    "                    accuracy = (predicted == Y_test_tensor).float().mean().item()\n",
    "                    cv_acc.append(accuracy)\n",
    "            avg_acc = np.mean(cv_acc)\n",
    "            \n",
    "            results.append([subj,avg_acc, cv_acc])\n",
    "    \n",
    "    return results"
   ],
   "id": "80a70673e278afb3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "results = adaptive_train(modelpath, subjs, scheme=4)",
   "id": "7d642ddb6a682cb0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 标记完成\n",
    "print('adapt model training is done')"
   ],
   "id": "f4f35f3a52811d28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(results)",
   "id": "63f6a81778032da8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 标记完成\n",
    "print('all done')"
   ],
   "id": "af6cb00966f6ffb7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T03:12:39.184665Z",
     "start_time": "2025-02-07T03:12:39.181186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sumup = 0\n",
    "for i in range(len(results)):\n",
    "    sumup += results[i][1]\n",
    "    print(f'avg acc for subj{i+1}: ',results[i][1])\n",
    "print('final acc for all subjs: ',sumup/len(results))"
   ],
   "id": "cc787d64e6901b2f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg acc for subj1:  0.825\n",
      "avg acc for subj2:  0.825\n",
      "avg acc for subj3:  0.8\n",
      "avg acc for subj4:  0.875\n",
      "avg acc for subj5:  0.9\n",
      "avg acc for subj6:  0.775\n",
      "avg acc for subj7:  0.775\n",
      "avg acc for subj8:  0.925\n",
      "avg acc for subj9:  0.9\n",
      "avg acc for subj10:  0.725\n",
      "avg acc for subj11:  0.8\n",
      "avg acc for subj12:  0.9\n",
      "avg acc for subj13:  0.775\n",
      "avg acc for subj14:  0.825\n",
      "avg acc for subj15:  0.975\n",
      "avg acc for subj16:  0.725\n",
      "avg acc for subj17:  0.725\n",
      "avg acc for subj18:  0.775\n",
      "avg acc for subj19:  0.85\n",
      "avg acc for subj20:  0.8\n",
      "avg acc for subj21:  0.875\n",
      "avg acc for subj22:  0.95\n",
      "avg acc for subj23:  0.875\n",
      "avg acc for subj24:  0.825\n",
      "avg acc for subj25:  0.925\n",
      "avg acc for subj26:  0.75\n",
      "avg acc for subj27:  0.875\n",
      "avg acc for subj28:  0.875\n",
      "avg acc for subj29:  0.9\n",
      "avg acc for subj30:  0.85\n",
      "avg acc for subj31:  0.875\n",
      "avg acc for subj32:  0.825\n",
      "avg acc for subj33:  0.925\n",
      "avg acc for subj34:  0.875\n",
      "avg acc for subj35:  0.85\n",
      "avg acc for subj36:  0.85\n",
      "avg acc for subj37:  0.9\n",
      "avg acc for subj38:  0.85\n",
      "avg acc for subj39:  0.75\n",
      "avg acc for subj40:  0.875\n",
      "avg acc for subj41:  0.775\n",
      "avg acc for subj42:  0.85\n",
      "avg acc for subj43:  0.85\n",
      "avg acc for subj44:  0.925\n",
      "avg acc for subj45:  0.9\n",
      "avg acc for subj46:  0.8\n",
      "avg acc for subj47:  0.85\n",
      "avg acc for subj48:  0.825\n",
      "avg acc for subj49:  0.925\n",
      "avg acc for subj50:  0.875\n",
      "final acc for all subjs:  0.8464999999999999\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a7065377ef459815"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
